{
 "metadata": {
  "name": "post_pilot12_13_stats"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import scipy as sp\n",
      "import numpy as np\n",
      "import csv, os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Introduction\n",
      "\n",
      "The plan is to read in a bunch of fields from our various data sources and then do some stats on them. These fields are:\n",
      "\n",
      "+ hashed_worker_id\n",
      "+ question_code\n",
      "+ num_answers_requested\n",
      "+ answer_num\n",
      "+ answer\n",
      "+ originality_fil\n",
      "+ originality_mike\n",
      "+ originality_inverse_sum_similarity\n",
      "+ originality_inverse_cluster_size\n",
      "+ fixed_cluster_number\n",
      "+ utility_fil\n",
      "+ utility_mike\n",
      "+ realistic_fil\n",
      "+ realistic_mike\n",
      "+ underdefined_fil\n",
      "+ underdefined_mike\n",
      "\n",
      "Note that the first four fields together provide a unique way of identifying the answer (this could be better).\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "base_data_dirs = ['/processed_data/pilot13',\n",
      "                  '/processed_data/pilot12',\n",
      "                  '/processed_data/pilot11',]\n",
      "\n",
      "fixed_cluster_csvs = map(lambda x: '/processed_data/pilot_upto_13_mixed/' + x + '_clusters.csv',\n",
      "                         ['charity', 'iPod', 'turk', 'forgot_name'])\n",
      "\n",
      "originality_csvs = map(lambda x: '/processed_data/pilot_upto_13_mixed/' + x + '_originality.csv',\n",
      "                         ['charity', 'iPod', 'turk', 'forgot_name'])\n",
      "\n",
      "manual_csvs = map(lambda x: '/processed_data/pilot_upto_13_mixed/' + x + '-scores.csv',\n",
      "                         ['fil', 'mike'])\n",
      "\n",
      "\n",
      "# Read in a bunch of base data\n",
      "\n",
      "def read_files(fils):\n",
      "    rows = []\n",
      "    for f in fils:\n",
      "        rows = rows + read_file(f)\n",
      "    return rows\n",
      "     \n",
      "\n",
      "def read_base_data(dirs):\n",
      "    rows = []\n",
      "    for d in dirs:\n",
      "      for f in os.listdir(d):\n",
      "        full_name = os.path.join(d,f)\n",
      "        if os.path.isdir(full_name):\n",
      "            print \"I deleted the code here\"\n",
      "        else:\n",
      "          if f == 'answers.csv':\n",
      "            rows = rows + read_file(full_name)\n",
      "    return rows\n",
      "\n",
      "def read_file(f):\n",
      "    with open(f) as fin:\n",
      "        l = fin.readline()\n",
      "        # If we see the follow, it's an early run we can ignore\n",
      "        if 'Number of Answers' in l:\n",
      "            return\n",
      "        fin.seek(0)\n",
      "        sep = '|'\n",
      "        # Guess separator character\n",
      "        if l.count(',') > l.count('|'):\n",
      "            sep = ','\n",
      "            \n",
      "        # Just cache for now\n",
      "        rows = []\n",
      "        for row in csv.reader(fin, delimiter=sep):\n",
      "            if 'hashed_worker_id' in row or 'worker_id' in row:\n",
      "                continue\n",
      "            rows.append(row)\n",
      "            \n",
      "        return rows\n",
      "\n",
      "# Read in the main processed data files\n",
      "all_rows = read_base_data(base_data_dirs)\n",
      "num_responses = len(all_rows)\n",
      "\n",
      "print \"num base responses:\", num_responses\n",
      "\n",
      "df_base = pd.DataFrame({'hashed_worker_id': pd.Series([row[0] for row in all_rows], dtype=object),\n",
      "            'question_code': pd.Series([row[2] for row in all_rows], dtype=object),\n",
      "            'num_requested': pd.Series([row[5] for row in all_rows], dtype=uint8),\n",
      "            'answer_num': pd.Series([row[6] for row in all_rows], dtype=uint8),\n",
      "            'answer': pd.Series([row[7] for row in all_rows], dtype=object)})\n",
      "\n",
      "# Read in manual codes\n",
      "\n",
      "def clean_missing(v):\n",
      "    return 0 if v == '' else v\n",
      "    \n",
      "def series_from_row(rows, index, t):\n",
      "    if t == object:\n",
      "        return pd.Series([row[index] for row in rows], dtype=t)\n",
      "    else:\n",
      "        return pd.Series([clean_missing(row[index]) for row in rows], dtype=t)\n",
      "\n",
      "def read_manual_csv(name):\n",
      "    index = 0 if name == 'fil' else 1\n",
      "    rows = read_file(manual_csvs[index])\n",
      "    \n",
      "    df = pd.DataFrame({'hashed_worker_id': series_from_row(rows, 0, object),\n",
      "            'question_code': series_from_row(rows, 2, object),\n",
      "            'num_requested': series_from_row(rows, 5, uint8),\n",
      "            'answer_num': series_from_row(rows, 6, uint8),\n",
      "            ('originality_%s' % name): series_from_row(rows, 12, uint8),\n",
      "            ('utility_%s' % name): series_from_row(rows, 13, uint8),\n",
      "            ('realistic_%s' % name): series_from_row(rows, 14, uint8),\n",
      "            ('underdefined_%s' % name): series_from_row(rows, 15, uint8),\n",
      "            ('distance_%s' % name): series_from_row(rows, 16, uint8),\n",
      "    })\n",
      "    return df\n",
      "    \n",
      "df_mike = read_manual_csv('mike')\n",
      "df_fil = read_manual_csv('fil')\n",
      "\n",
      "print \"num mike responses:\", len(df_mike)\n",
      "print \"num fil responses:\", len(df_fil)\n",
      "\n",
      "# Read in cluster codes\n",
      "\n",
      "all_rows = read_files(fixed_cluster_csvs)\n",
      "df_clusters = pd.DataFrame({'hashed_worker_id': series_from_row(all_rows, 3, object),\n",
      "                            'question_code': series_from_row(all_rows, 6, object),\n",
      "                            'answer_num': series_from_row(all_rows, 2, uint8),\n",
      "                            'num_requested': series_from_row(all_rows, 5, uint8),\n",
      "                            'fixed_cluster_number': series_from_row(all_rows, 0, uint8),\n",
      "})\n",
      "\n",
      "print \"num clustered responses:\", len(df_clusters)\n",
      "\n",
      "# Read in originality scores\n",
      "\n",
      "all_rows = read_files(originality_csvs)\n",
      "\n",
      "df_originality = pd.DataFrame({'hashed_worker_id': series_from_row(all_rows, 5, object),\n",
      "                            'question_code': series_from_row(all_rows, 8, object),\n",
      "                            'answer_num': series_from_row(all_rows, 4, uint8),\n",
      "                            'num_requested': series_from_row(all_rows, 7, uint8),\n",
      "                            'originality_inverse_sum_similarity': series_from_row(all_rows, 2, float64),\n",
      "                            'originality_inverse_cluster_size': series_from_row(all_rows, 3, float64),\n",
      "})\n",
      "\n",
      "print \"num originality responses:\", len(df_originality)\n",
      "\n",
      "merge_column_names = ['hashed_worker_id', 'question_code', 'answer_num', 'num_requested']\n",
      "\n",
      "df = pd.merge(df_base, df_mike, 'outer', merge_column_names)\n",
      "df = pd.merge(df, df_fil, 'outer', merge_column_names)\n",
      "df = pd.merge(df, df_clusters, 'outer', merge_column_names)\n",
      "df = pd.merge(df, df_originality, 'outer', merge_column_names)\n",
      "\n",
      "print \"number merged respones\", len(df)\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "num base responses: 2227\n",
        "num mike responses:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1225\n",
        "num fil responses: 1225\n",
        "num clustered responses: 2227\n",
        "num originality responses:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2227\n",
        "2227\n",
        "2227\n",
        "2227\n",
        "2227\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}