\section{Goals and Hypotheses}


% Researchers have long debated how to best assess creativity in a way that consistently captures the complex dimensions of creativity [35]. The most commonly adopted approach [1, 5, 17] and the one we employ is to ask domain experts to subjectively make scale-rated assessments of creativity. Typically, domain experts rate originality, feasibility, elaboration, and flexibility and count the number of ideas generated [3].

To characterize brainstorming in microtask marketplaces, we are interested in modeling rates at which ideas and idea categories are generated, the originality of ideas as measured by o-scores, and individuals' idea generation processes.

%Our primary motivation is to develop baseline for brainstorming performance in crowd marketplaces. Brainstorming performance is often measured in terms of creativity. However, there any many interpretations of creativity [CITE a couple citations]. Instead we are interested in performance on variables that are commonly \emph{components} of a creativity score: quantity, originality, and surprise factor.

%As our primary goal, we examine these outcomes with respect to the number of answer requested from workers. This is the simplest of conditions that can vary when choosing a design for a crowd brainstorming problem. Furthermore, we detail the phenomenon of \emph{riffing} (known in the idea generation literature as \emph{clustering}, a term we must unfortunately reserve for it's classification-related meaning).

In modeling these quantities, we explore the following hypotheses:

\textbf{Hypothesis 1:} The probability that an idea is novel decreases non-linearly as a function of the number of responses received. Similarly, we expect the rate of new idea categories to decrease non-linearly over time.

\textbf{Hypothesis 2:} There is a set of general, common ideas that make up the first several responses of every crowd brainstorming session. In other words, we expect there to be a relatively fixed set of ideas that most people think of first.

%Parnes et al \cite{parnes_effects_1961} showed that more high quality ideas are generated in the latter half of a brainstorming run than the former half. Because of the difficulty in reliably assessing the quality of an idea for a brainstorming exercise, we instead consider the o-score of an idea to indirectly measure this concept.

%\textbf{Hypothesis 3:} O-scores generated in the second half of a brainstorming session will be higher than ideas generated in the first half. This hypothesis provides a complementary perspective to the former.

With respect to individual brainstorming processes, we have a pair of hypotheses derived from Nijstad and Stroebe's work and their SIAM model \cite{nijstad_how_2006}:

\textbf{Hypothesis 3:} An idea from one semantic category of ideas should more often be followed by an idea from the same category than expected by random chance.

\textbf{Hypothesis 4:} The time it takes to generate an idea should be longer when changing semantic categories than when generating ideas within a category.


