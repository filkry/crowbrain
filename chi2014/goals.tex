\section{Goals and Hypotheses}

Researchers have used different metrices to assess the brainstorming process and its outputs. Typically, the measures of concern are the novelty of generated ideas, the practicality/appropriateness of ideas, the variety of ideas (flexibility), and quantity of ideas generated. \cite{finke1992creative, shah2003metrics}. The most common approach to measure novelty and appropriateness is to ask domain experts to subjectively make scale-rated assessment of each \cite{lewis2011affective, amabile_1983}. Depending on the types of artifacts and researchers' interest, these two measures are sometimes combined into one measure of quality \cite{little2010exploring}. Instead of subjective ratings, some researchers opt for a measure that can be derived objectively. For example, Jansson et al. used `o' scores, derived from the frequency an idea occur, to evaluate products novelty \cite{jansson_design_1991}. Flexibility is measured by the number of explored ``categories'' in a design space or by subjective rating from domain experts. \cite{lewis2011affective, marsh1996examples}. Category or feature-based quantities also yield other metrics such as within-catgories fluency and  the length of train-of-thought when participants focus on a particular category \cite{nijstad_how_2006}.

% Researchers have long debated how to best assess creativity in a way that consistently captures the complex dimensions of creativity [35]. The most commonly adopted approach [1, 5, 17] and the one we employ is to ask domain experts to subjectively make scale-rated assessments of creativity. Typically, domain experts rate originality, feasibility, elaboration, and flexibility and count the number of ideas generated [3].

To characterize brainstorming in microtask marketplaces, we are interested in modeling the following quantities:
\begin{itemize}
\item the rate of unique idea production
\item the rate of production of unique \emph{categories} of ideas
\item the relative originality of ideas
\item individual practices in idea generation
\end{itemize}

We will examine these rates as a function of the number of ideas requested from workers.

%Our primary motivation is to develop baseline for brainstorming performance in crowd marketplaces. Brainstorming performance is often measured in terms of creativity. However, there any many interpretations of creativity [CITE a couple citations]. Instead we are interested in performance on variables that are commonly \emph{components} of a creativity score: quantity, originality, and surprise factor.

%As our primary goal, we examine these outcomes with respect to the number of answer requested from workers. This is the simplest of conditions that can vary when choosing a design for a crowd brainstorming problem. Furthermore, we detail the phenomenon of \emph{riffing} (known in the idea generation literature as \emph{clustering}, a term we must unfortunately reserve for it's classification-related meaning).

In modeling these quantities, we have several hypotheses related to results received in the aggregate (across all participants), as well as individuals' brainstorming processes.

\textbf{Hypothesis 1:} The rate of new ideas will diminish exponentially toward zero as a function of the number of responses gathered. Less formally, by continually soliciting solutions, we will eventually reach a saturation point at which any idea a worker submits is likely to be the same or practically identical to previously seen solutions. Related to this, we expect the number of \emph{categories} of ideas to diminish exponentially as well.

\textbf{Hypothesis 2:} The rate of new ideas will tend toward zero faster in some conditions than others. In particular, we expect conditions requesting a smaller number of ideas will tend toward zero faster, for reasons hypothesized next.

\textbf{Hypothesis 3:} We anticipate there is a set of general, common ideas that make up the first several responses of every crowd brainstorming session, regardless of condition. In other words, we expect there to be a relatively fixed set of ideas that ``everyone'' thinks of first.

Parnes et al \cite{parnes_effects_1961} showed that more high quality ideas are generated in the latter half of a brainstorming run than the former half. Because of the difficulty in reliably assessing the quality of an idea for a brainstorming exercise, we instead consider the o-score of an idea to indirectly measure this concept.

\textbf{Hypothesis 4:} Ideas generated in the second half of a brainstorming session have higher o-scores than ideas generated in the first half.

With respect to individual brainstorming processes, we have several hypotheses derived from Nijstad and Stroebe's work and their SIAM model \cite{nijstad_how_2006}:

\textbf{Hypothesis 5:} An idea from one semantic category of ideas should more often be followed by an idea from the same category than expected by random chance.

\textbf{Hypothesis 6:} The time it takes to generate an idea should be longer when changing semantic categories than when generating ideas within a category.


