\section{Related Work}
In this section, we review related work in the areas of brainstorming and crowdsourcing.

\subsection{Traditional Brainstorming}
In the 1950s, Osborne formalized the brainstorming process, providing a set of recommendations for idea generation by groups \cite{osborn_applied_1957}: \emph{focus on quantity}, \emph{withhold criticism}, \emph{welcome unusual ideas}, and \emph{combine and improve ideas}. These recommendations form the basis for the instructions we provide to workers in our experiments.

Following Osborne's work, Taylor et al.\ established that brainstorming by \emph{nominal groups}, or individuals working in isolation from one another, yields better results than collocated groups in terms of number of ideas generated \cite{taylor_does_1958}. These effects can be partially explained by the absence of counterproductive social forces, such as fear of judgment. Bouchard and Hare \cite{bouchard_jr_size_1970} later found that nominal brainstorming groups are able to generate quantities of ideas linear in the number of members in the group.

Electronic brainstorming is a variant in which individuals brainstorm at separate computer terminals, with the exchange of ideas performed over the network \cite{gallupe_electronic_1992}. Participants enter ideas while others' ideas are anonymously presented on-screen. As with nominal brainstorming, Gallupe \cite{gallupe_electronic_1992} observed that electronic brainstorming can reduce counterproductive social effects. However, Pinsonneault et al.\ \cite{pinsonneault_electronic_1999} identify productivity impediments introduced by group electronic brainstorming, such as being distracted by other ideas appearing, and individuals witholding ideas they feel are derivative of others. They find little evidence to support the proposition that electronic brainstorming outperforms nominal brainstorming.

Finally, Diehl and Stroebe found that groups that generate many ideas also generate \emph{good} ideas \cite{diehl_productivity_1987}, a finding that has been verified many times \cite{briggs1997quality, parnes1959effects, parnes_effects_1961, shah2003metrics, cross1996creativity}.

Shifting our focus to an \emph{individual's} brainstorming processes, Nijstad and Stroebe provide a model for individual idea generation dubbed SIAM, or the ``search for ideas in associative memory'' \cite{nijstad_how_2006}. SIAM defines two stages of idea generation: the generation of ideas based on working memory, and idea production within a single activated ``image'' (where an image can be thought of as a category of ideas). This model makes predictions, one of the most important being that an idea is more likely to be followed by an idea in the same category than would be expected by chance. Their model also predicts that new ideas in the same category as the previous idea are generated faster than ideas between categories. Nijstad and Stroebe verified these predictions in the context of nominal brainstorming.

To assess the brainstorming process and its outputs, a number of measures have been proposed. Typically, the measures of concern are the novelty of generated ideas, the practicality/appropriateness of ideas, the variety of ideas (flexibility), and the quantity of ideas generated. \cite{finke1992creative, shah2003metrics}.

The most common approach to measure novelty and appropriateness is to ask domain experts to subjectively make scale-rated assessment of each \cite{lewis2011affective, amabile_1983}. These two measures are sometimes combined into one measure of quality \cite{little2010exploring}.
Some researchers opt for a measure that can be derived objectively. For example, Jansson et al. used `o' scores, derived from the frequency with which an idea occurs, to evaluate novelty \cite{jansson_design_1991}. We adopt this use of o-score in our later analyses.

Flexibility is often measured by the number of explored ``categories'' in a design space, or by subjective rating from domain experts \cite{lewis2011affective, marsh1996examples}. Category or feature-based quantities yield metrics such as within-categories fluency and length of train-of-thought when participants focus on a particular category \cite{nijstad_how_2006}. Building in this past line of work, we consider both individual ideas and categories of ideas.

\subsection{Crowdsourced Brainstorming}
%The simplest imaginable instantiation of brainstorming in a microtask marketplace mirrors that of nominal brainstorming. More sophisticated environments are possible, such as those that forward ideas that others have generated (electronic brainstorming). 
%Examples for these environments are platforms for generating social innovation ideas or product concepts like OpenIDEO, Quirky and Threadless that employ structured process where participants take on different responsibility from generating ideas to evaluating others' ideas.
We constrain our focus to nominal brainstorming in this environment with the goal of establishing baselines.

While we are unaware of specific studies of brainstorming in microtask marketplaces, there is a range of related research examining other creative processes \cite{lewis2011affective, kittur2011crowdforge, Zhang:2012:HCT:2207676.2207708}. For example, Yu and Nickerson created an evolutionary crowd algorithm to generate creative chair designs \cite{yu_cooks_2011}. Their work demonstrates the creative potential of the crowd, but does not specifically address brainstorming or processes in which participants generate many ideas. 

Little et el. compared the iterative and parallel human computation processes on a brainstorming task for names for fabricated companies \cite{little2010exploring}. In the iterative condition, participants saw all names suggested by previous participants. Participants in the parallel condition worked in isolation without seeing other participants' ideas. The parallel process was found to be more likely to generate names with high ratings, advocating the case for nominal brainstorming. The study did not mention categorization of generated ideas, an important measure on flexibility of idea generation \cite{lewis2011affective, nijstad_how_2006, finke1992creative, shah2003metrics}, nor did it explore requesting more than five ideas.

The nature of the microtask marketplace may also require alternative ways of structuring the brainstorming activity.
% calls for an alternative structure for brainstorming tasks in this setting. Researchers have found that 30\% or more submission on Mechanical Turk may be low quality \cite{kittur2008crowdsourcing}. For example, submissions to a brainstorming task for a company name are sometimes grammartically awkward or offensive \cite{little2010exploring}. These low quality responses were not found in walk-in brainstorming studies. 
For example, Bernstein et al. found high variance of effort from Amazon Mechanical Turk workers \cite{soylent}. They characterized two worker personas at opposite ends of the effort spectrum: the {\em Eager Beaver\/} who goes beyond task requirement and the {\em Lazy Turker\/} who does as little work as necessary. 

 %The motivational structure of microstask marketplace does not map well with most previous work on brainstorming where the sessions are time-limited. 


%TODO: More crowdsourcing related work here. Maybe the Little iterative vs parallel paper? It includes creative tasks. Perhaps describe some of the unique characteristics of microtask marketplaces, like people's desire to perform a task and move on...

In summary, while brainstorming has been examined in traditional and electronic settings, its application in crowd marketplaces has received limited attention, leaving open questions.
