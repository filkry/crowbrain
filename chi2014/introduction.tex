%Whether in research or industry, productivity can be hamstrung by the need to generate creative or novel ideas. Given the ubiquity of idea generation tasks, it is desireable to provide generalizable automated methods of solving them. Automated computational methods to solve specific problems are unlikely to appear without significant advances in natural language processing and artificial intelligence. As a result, we are left with human-centric solutions, such as brainstorming.

Microtask marketplaces such as Amazon's Mechanical Turk have proven to be useful platforms for outsourcing a wide range of tasks, from proofreading documents \cite{soylent} to estimating the caloric content of food \cite{platemate}.  These platforms make possible human computation systems \cite{quinn_2011}, hybrid systems that leverage humans' abilities to solve problems that currently cannot be solved by machines alone.

A particularly compelling use of microtask marketplaces is to tap into the collective creativity of crowds. This enables one to quickly sample a wide range of ideas from a diverse population, which is useful for tasks such as brainstorming or design. Aaron Koblin's art piece ``The Sheep Market'' \cite{koblin_2009} was an early demonstration of this potential. In this project, he asked 10,000 individuals to draw a ``sheep facing left,'' yielding a wide range of unique results. More recently, there has been interest in systems that support crowd-based design. For example, Yu and Nickerson developed a system that combines genetic algorithms with design input from crowd workers to explore a design space \cite{yu_cooks_2011}. 

In this paper, we focus on crowd-based brainstorming. While brainstorming has been an implicit component of many creative endeavors in microtask marketplaces, we are unaware of any research that specifically describes and models the brainstorming process in this environment. Furthermore, while there is a rich literature describing brainstorming processes of individuals and groups (e.g., \cite{osborn_applied_1957, pinsonneault_electronic_1999, nijstad_how_2006,brown1998modeling}), there are important qualitative differences in microtask marketplaces. For example, crowd workers exhibit high variance of effort in completing a task \cite{soylent}. Brainstorming participants in a microtask marketplace are also paid for their participation, and some might be more eager than others to complete a task as fast as possible to maximize earnings \cite{kittur2008crowdsourcing}. Furthermore, participants in microtask marketplaces cannot be expected to be spatially or temporally co-located. Collectively, these differences make it unclear how brainstorming unfolds in this context.

%Brainstorming is a tool proposed by Osborne \cite{osborn_applied_1957} that has achieved widespread popularity and similarly widespread research and controversy. Brainstorming methods have been applied in many scenarios, from spatially and temporally co-located participants to electronic brainstorming over computer networks \cite{pinsonneault_electronic_1999}. The introduction of crowd marketplaces such as Amazon's Mechanical Turk \cite{_amazon_????} and the corresponding paradigm of massively parallel Human Intelligence Tasks provide both a natural venue to solicit ideas and a new context in which to establish models of brainstorming activity.

%This setting introduces unique challenges. For example, crowd marketplaces are driven by financial transactions, while participants in traditional brainstorming settings are required and motivated to generate ideas for a given duration. Guidelines and points of comparison are needed.

% In the course of their research in Human-Computer Interactions, two of the authors went looking for these sets of guidelines to apply them to a crowd idea generation task. While previous work has utilized brainstorming techniques in crowd marketplaces [CITE], we were unable to find any generalizable models or guidelines for brainstorming in the context of the crowd. Models such as Nijstaf and Stroebe's SIAM \cite{nijstad_how_2006} make and test predictions regarding in-person brainstorming, but we did not find similar baselines for electronic or crowd brainstorming.

This paper presents the results of a series of experiments intended to model brainstorming in microtask marketplaces (specifically, Amazon's Mechanical Turk). In our experiments, we asked individuals to brainstorm ways to use an MP3 player whose batteries were dead. We varied the number of ideas requested (5, 10, 20, 50, 75, 100, or unlimited), paying a fixed amount per idea (3.5 cents). Across all conditions, 154 workers submitted responses to 170 HITs (Human Intelligence Tasks), yielding 3292 ideas. From these ideas, we present a series of models characterizing features of crowd brainstorming, such as the rate of novel idea production, the rate at which novel {\em categories\/} of ideas are produced (where a category represents a set of conceptually similar ideas that are distinct from other categories), qualities of the ideas (for example, whether they are common or not), and characteristics of the brainstorming process itself.

Our primary findings and contributions are as follows:
\begin{itemize}
\item The rates at which novel ideas and categories of ideas are produced follow logarithmic growth models. Thus, the rates of novel ideas and categories of ideas decrease exponentially.
%\item The rates for generating novel ideas changes with the number of responses requested per individual, even if the overall number of responses is held constant. In general, a higher rate of novel idea generation is achieved by asking fewer participants for more responses each. This result corresponds to previous finding that making participants to extend more effort generating ideas--in this case by requiring them to generate a lot of ideas--will lead to an increasing proportion of good ideas\cite{parnes_effects_1961}.
%[TODO: There is likely past work that has a similar finding; we should relate this finding to it. Taylor Berry Block as related? Diehl as related?][Pao's note: can't find more directly related stuff mainly because most of the previous lit focus on time limit task rather than number of ideas. This is actually a good argument for the paper since time limit might not work in this marketplace.]
\item People are likely to come up with the same overall set of ideas early in their brainstorming process, but then diverge from this set of common ideas as they generate more ideas. Given our results, we find there is little value in requesting fewer than 20 ideas from workers, since these early ideas are among the most common.
\item Individuals' patterns of idea generation mimic those identified by Nijstaf and Stroebe's SIAM (search for ideas in associative memory) model \cite{nijstad_how_2006}: Workers produce a set of ideas from working memory, then switch strategies to generate ideas that are permutations of other ideas, a process we dub {\em riffing\/}. In our data, we see the prevalence of riffing steadily increase across the first 20 responses, after which its prevalence plateaus.
\item We also replicate Nijstaf and Stroebe's findings with respect to the time it takes to generate a new idea \cite{nijstaf_how_2006}.
\item Requesting ``as many ideas as possible'' for a fixed price (fixed for the entire HIT, rather than per idea) is inadvisable, as it leads to a relatively low number of ideas produced (in our experiments, fewer than 15).
\end{itemize}

%Overall, across a number of metrics and perspectives (e.g., where novel ideas show up in the brainstorming process, time spent producing ideas), crowd brainstorming shares a number of similarities with brainstorming in more traditional environments.

Our results provide baseline models useful for two audiences: 1) those who wish to leverage crowds for brainstorming, and 2) researchers interested in developing techniques to optimize the crowd brainstorming processes. For the former, our models provide clear guidance and recommendations for various parameters of brainstorming in this environment (such as the number of questions to request from workers). For the latter group, our models provide a baseline for comparing novel techniques intended to increase the quality of brainstorming output by crowds.

%We provide a method for clustering responses to idea generation problems and provide a detailed description of the resulting data set, which has been made publically available. We demonstrate the saturation of ideas received over time, and show that the rate of saturation, and size of the saturated idea set, originality of ideas and generality of ideas vary between conditions.

%Furthermore, we examine predictions made by Nijstad and Stroebe's model of in-person brainstorming and test them in the context of crowd brainstorming. We find that the tested predictions hold: that ideas generated in sequence tend to belong to the same category, and that the time spent to generate when changing categories is signficantly greater than when generating ideas within the same category.

%Finally, we briefly outline qualitative findings in the data, and recommend future work motivated by these findings.

In the rest of this paper, we review prior work, then describe our experimental design, results from our experiments, and implications for practitioners and researchers.
