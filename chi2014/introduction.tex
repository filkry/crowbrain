%Whether in research or industry, productivity can be hamstrung by the need to generate creative or novel ideas. Given the ubiquity of idea generation tasks, it is desireable to provide generalizable automated methods of solving them. Automated computational methods to solve specific problems are unlikely to appear without significant advances in natural language processing and artificial intelligence. As a result, we are left with human-centric solutions, such as brainstorming.

Microtask marketplaces such as Amazon's Mechanical Turk have proven to be useful platforms for outsourcing a wide range of tasks, from proofreading documents \cite{soylent} to estimating the caloric content of pictures of food \cite{platemate}.  These platforms make possible human computation systems \cite{quinn_2011}, hybrid computational systems that tap humans' abilities to solve problems that currently cannot be solved by machines alone.

A particularly compelling use of microtask marketplaces is to tap into the collective creativity of crowds. This enables one to quickly sample a wide range of ideas from a diverse population, which is useful for tasks such as brainstorming or design. Aaron Koblin's art piece ``The Sheep Market'' \cite{koblin_2009} was an early demonstration of this potential. In this project, he asked 10,000 individuals to draw a ``sheep facing left,'' yielding a wide range of unique results. More recently, there has been interest in systems that support crowd-based design. For example, Yu and Nickerson developed a system that combines genetic algorithms with design input from crowd workers to explore a design space \cite{yu_cooks_2011}. 

In this paper, we focus on crowd-based brainstorming. While brainstorming has been an implicit component of many creative endeavors in microtask marketplaces, we are unaware of any research that specifically describes and models the brainstorming process in this environment. Furthermore, while there is a rich literature describing brainstorming processes of individuals and groups (e.g., \cite{osborn_applied_1957, pinsonneault_electronic_1999, nijstad_how_2006} OTHERS?), microtask marketplaces are qualitatively different from previously studied environments, calling into question how much of this past research maps onto this environment. For example, brainstorming participants in a microtask marketplace are paid for their participation, and are often eager to complete a task as fast as possible to move on to the next paying task [REF?]. Given these and other differences, there are many open questions about how brainstorming unfolds in this context.

%Brainstorming is a tool proposed by Osborne \cite{osborn_applied_1957} that has achieved widespread popularity and similarly widespread research and controversy. Brainstorming methods have been applied in many scenarios, from spatially and temporally co-located participants to electronic brainstorming over computer networks \cite{pinsonneault_electronic_1999}. The introduction of crowd marketplaces such as Amazon's Mechanical Turk \cite{_amazon_????} and the corresponding paradigm of massively parallel Human Intelligence Tasks provide both a natural venue to solicit ideas and a new context in which to establish models of brainstorming activity.

%This setting introduces unique challenges. For example, crowd marketplaces are driven by financial transactions, while participants in traditional brainstorming settings are required and motivated to generate ideas for a given duration. Guidelines and points of comparison are needed.

% In the course of their research in Human-Computer Interactions, two of the authors went looking for these sets of guidelines to apply them to a crowd idea generation task. While previous work has utilized brainstorming techniques in crowd marketplaces [CITE], we were unable to find any generalizable models or guidelines for brainstorming in the context of the crowd. Models such as Nijstaf and Stroebe's SIAM \cite{nijstad_how_2006} make and test predictions regarding in-person brainstorming, but we did not find similar baselines for electronic or crowd brainstorming.

This paper presents results from a series of experiments intended to model brainstorming on microtask marketplaces (specifically, Amazon's Mechanical Turk). In our experiments, we asked individuals to brainstorm ways to use an MP3 player whose batteries were dead. We varied the number of ideas requested (5, 10, 20, 50, 75, 100, or unlimited), paying a fixed amount per idea (3.5 cents). Across all conditions, 154 workers submitted responses to 170 HITs (Human Intelligence Tasks), yielding 3292 ideas. From these ideas, we present a series of models characterizing features of crowd brainstorming, such as the rate of novel idea production, the rate at which novel {\em categories\/} of ideas are produced (where a category represents a set of conceptually similar ideas that are distinct from other categories), and the relative novelty of ideas, all as functions of number of responses requested.

Our primary findings and contributions are as follows:
\begin{itemize}
\item The rates at which novel ideas and categories of ideas are produced follow logarithmic growth models, as a function of the total number of responses received, from all participants. In other words (and as one would expect), there are diminishing returns from asking more people to submit ideas. However, the actual rates differ per condition, as described below
\item The rates for generating novel ideas are linked to the number of responses requested: The more responses requested from individuals, the higher the rate of novel idea generation, in general. [TODO: There is likely past work that has a similar finding; we should relate this finding to it. Taylor Berry Block as related? Diehl as related?]
\item People are likely to come up with the same overall set of ideas early in their brainstorming process, but then diverge from this set of common ideas as they generate more ideas. Our data suggest that the first 20 ideas are the most unoriginal (i.e., the most common). This finding complements the previous finding, and further suggests that there is little value in requesting fewer than 20 ideas from workers
\item Individuals' patterns of idea generation mimic those identified by Nijstaf and Stroebe's SIAM (search for ideas in associative memory) model \cite{nijstad_how_2006}: Workers produce a set of ideas from working memory, then switch strategies to generate ideas that are permutations of other ideas, a process we dub {\em riffing\/}. In our data, we see the prevalence of riffing steadily increase over the first 20 responses, after which its prevalence plateaus
\item Requesting ``as many ideas as possible'' for a fixed price (fixed for the entire HIT, rather than per idea) is inadvisable, as it leads to a relatively low number of ideas produced (in our experiments, fewer than 20)
\item The time spent producing an idea matches the times reported in previous studies \cite{nijstad_how_2006}
\end{itemize}

%Overall, across a number of metrics and perspectives (e.g., where novel ideas show up in the brainstorming process, time spent producing ideas), crowd brainstorming shares a number of similarities with brainstorming in more traditional environments.

Our results provide baseline models useful for two audiences: 1) those who wish to leverage crowds for brainstorming, and 2) researchers interested in developing techniques to optimize the crowd brainstorming processes. For the former, our models provide clear guidance and recommendations for various parameters of brainstorming in this environment (such as the number of questions to request from workers). For the latter group, our models provide a baseline for comparing novel techniques intended to increase the quality of brainstorming output by crowds.

%We provide a method for clustering responses to idea generation problems and provide a detailed description of the resulting data set, which has been made publically available. We demonstrate the saturation of ideas received over time, and show that the rate of saturation, and size of the saturated idea set, originality of ideas and generality of ideas vary between conditions.

%Furthermore, we examine predictions made by Nijstad and Stroebe's model of in-person brainstorming and test them in the context of crowd brainstorming. We find that the tested predictions hold: that ideas generated in sequence tend to belong to the same category, and that the time spent to generate when changing categories is signficantly greater than when generating ideas within the same category.

%Finally, we briefly outline qualitative findings in the data, and recommend future work motivated by these findings.

In the rest of this paper, we review prior work, then describe our experimental design, results from our experiments, and implications for practitioners and researchers.