Whether in research or industry, productivity can be hamstrung by the need to generate creative or novel ideas. Given the ubiquity of idea generation tasks, it is desireable to provide generalizable automated methods of solving them. Automated computational methods to solve specific problems are unlikely to appear without significant advances in natural language processing and artificial intelligence. As a result, we are left with human-centric solutions, such as brainstorming.

Brainstorming is a tool proposed by Osborne \cite{osborn_applied_1957} that has achieved widespread popularity and similarly widespread research and controversy. Brainstorming methods have been applied in many scenarios, from spatially and temporally co-located participants to electronic brainstorming over computer networks \cite{pinsonneault_electronic_1999}. The introduction of crowd marketplaces such as Amazon's Mechanical Turk \cite{_amazon_????} and the corresponding paradigm of massively parallel Human Intelligence Tasks provide both a natural venue to solicit ideas and a new context in which to establish models of brainstorming activity.

This setting introduces unique challenges. For example, crowd marketplaces are driven by financial transactions, while participants in traditional brainstorming settings are required and motivated to generate ideas for a given duration. Guidelines and points of comparison are needed.

In the course of their research in Human-Computer Interactions, two of the authors went looking for these sets of guidelines to apply them to a crowd idea generation task. While previous work has utilized brainstorming techniques in crowd marketplaces [CITE], we were unable to find any generalizable models or guidelines for brainstorming in the context of the crowd. Models such as Nijstaf and Stroebe's SIAM \cite{nijstad_how_2006} make and test predictions regarding in-person brainstorming, but we did not find similar baselines for electronic or crowd brainstorming.

This paper and corresponding data set provide a baseline for the idea generation process in crowd marketplaces. We establish a corpus of brainstorming data in a restricted design space with two axes: idea generation problem, and number of responses requested of a single crowd worker.

We provide a method for clustering responses to idea generation problems and provide a detailed description of the resulting data set, which has been made publically available. We demonstrate the saturation of ideas received over time, and show that the rate of saturation, and size of the saturated idea set, originality of ideas and generality of ideas vary between conditions.

Furthermore, we examine predictions made by Nijstad and Stroebe's model of in-person brainstorming and test them in the context of crowd brainstorming. We find that the tested predictions hold: that ideas generated in sequence tend to belong to the same category, and that the time spent to generate when changing categories is signficantly greater than when generating ideas within the same category.

Finally, we briefly outline qualitative findings in the data, and recommend future work motivated by these findings.